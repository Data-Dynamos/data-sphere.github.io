---
sidebar_position: 4
---

# Data Milky Way: Brief History (Part 3) - Data Processing

<div style={{textAlign: 'center'}}>

<figure class="video-container">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/Uc-Wtem-lyw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</figure>

</div>


## Evolution of Data Processing
<div style={{textAlign: 'center'}}>

![map-reduce-processing.png](./assets/map-reduce-processing.png)
![data-processing-vision.png](./assets/data-processing-vision.png)

</div>

* Recap: [Why move from Hadoop to Spark + Object Storage](https://www.xplenty.com/blog/apache-spark-vs-hadoop-mapreduce/)
* [Intro to Metastores and Data Catalogs](https://lakefs.io/metadata-management-hive-metastore-vs-aws-glue/)
* [Batch and Micro-Batch Streaming](https://www.upsolver.com/blog/batch-stream-a-cheat-sheet)
* [Continuous Processing](https://hazelcast.com/glossary/stream-processing/)
* [One syntax to rule them all?](https://beam.apache.org/)
  * Apache Beam is based on the [Dataflow model introduced by Google](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43864.pdf)
  * Aims to unify the semantics of batch & streaming processing across engines (Flink, Spark, etc.)
  
**You donâ€™t necessarily need streaming, let alone Beam!** Evaluate your own projectâ€™s needs

In my experience, most teams actually simply choose either Spark Structured Streaming or Flink (without Beam)

## Orchestration Core Concepts
But how do we make our pipeline **flow**? ðŸŒŠ
* Data Engineering workflows often involve transforming and transferring data from one place to another
* Workflows in real-life have multiple steps and stages


* Sometimes, everything might work fine with just CRON jobs
* However, other times, you might want to control the state transitions of these steps:
  * e.g. if Step A doesnâ€™t run properly, donâ€™t run Step B because the data could be corrupt, instead run Step C
  * Once again, the concept of Directed Acyclic Graphs (DAGs) can come to our rescue

* [Apache Airflow](https://www.youtube.com/watch?v=XD7euLOzKbs&ab_channel=SFPython) is just one nice way of setting up DAGs to orchestrate jobs ðŸŒˆ
  * Note: Airflow is primarily designed as a task orchestration tool
  * You can trigger tasks on the Airflow cluster itself or on remote targets (e.g. AWS Fargate, Databricks, etc.)
  * NOT designed for transferring large amounts of actual data
  * [Reference](https://airflow.apache.org/docs/apache-airflow/1.10.1/#beyond-the-horizon)
  * [Play around with Airflow locally](https://github.com/kelseymok/airflow/)



## Practical Data Workloads

<div style={{textAlign: 'center'}}>

![big-data-sword.png](./assets/big-data-sword.png)

Weâ€™re here to teach you big data skills, but in reality...

</div>

### Single-Node vs. Cluster
Not everything is Big Data!
[You donâ€™t always need Spark](https://www.indellient.com/blog/a-journey-from-pandas-to-spark-data-frames/)
([sometimes Pandas deployed on a single node function/container is just fine!](https://www.indellient.com/blog/a-journey-from-pandas-to-spark-data-frames/))

### Batch vs Streaming
[Streaming isnâ€™t always the solution](https://www.section.io/engineering-education/batch-processing-vs-stream-processing/)!

### Orchestration options
**DAG-based approaches:** [Apache Airflow](https://airflow.apache.org/), [Databricks Jobs Orchestration](https://databricks.com/blog/2021/07/13/announcement-orchestrating-multiple-tasks-with-databricks-jobs-public-preview.html), [Dagster](https://dagster.io/)
**Event-Driven + Declarative** (e.g. [Databricks Auto Loader](https://databricks.com/discover/demos/delta-lake-data-integration-demo-auto-loader-and-copy-into), [Delta Live Tables](https://databricks.com/discover/demos/delta-live-tables-demo))
**Other triggers:** (e.g. AWS Lambda, [Glue Triggers](https://docs.aws.amazon.com/glue/latest/dg/trigger-job.html))



