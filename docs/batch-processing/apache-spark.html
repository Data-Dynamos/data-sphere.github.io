<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-batch-processing/apache-spark">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.0">
<title data-rh="true">Apache Spark |  DataSphere</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://data-dynamos.github.io/data-sphere.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://data-dynamos.github.io/data-sphere.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://data-dynamos.github.io/data-sphere.github.io/docs/batch-processing/apache-spark"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Apache Spark |  DataSphere"><meta data-rh="true" name="description" content="Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python, and R, and an optimised engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing."><meta data-rh="true" property="og:description" content="Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python, and R, and an optimised engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing."><link data-rh="true" rel="icon" href="/data-sphere.github.io/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://data-dynamos.github.io/data-sphere.github.io/docs/batch-processing/apache-spark"><link data-rh="true" rel="alternate" href="https://data-dynamos.github.io/data-sphere.github.io/docs/batch-processing/apache-spark" hreflang="en"><link data-rh="true" rel="alternate" href="https://data-dynamos.github.io/data-sphere.github.io/docs/batch-processing/apache-spark" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/data-sphere.github.io/blog/rss.xml" title=" DataSphere RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/data-sphere.github.io/blog/atom.xml" title=" DataSphere Atom Feed"><link rel="stylesheet" href="/data-sphere.github.io/assets/css/styles.5e85331d.css">
<link rel="preload" href="/data-sphere.github.io/assets/js/runtime~main.9b41e559.js" as="script">
<link rel="preload" href="/data-sphere.github.io/assets/js/main.9f669193.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/data-sphere.github.io/"><div class="navbar__logo"><img src="/data-sphere.github.io/img/datasphere_1.jpeg" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/data-sphere.github.io/img/datasphere_1.jpeg" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">DataSphere</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/data-sphere.github.io/docs/tour_agenda">Lessons</a><a class="navbar__item navbar__link" href="/data-sphere.github.io/resources">Resources</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/Data-Dynamos/data-dynamos.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/data-sphere.github.io/docs/tour_agenda">Tour Agenda</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/data-sphere.github.io/docs/category/intro-to-data-engineering">Intro to Data Engineering</a><button aria-label="Toggle the collapsible sidebar category &#x27;Intro to Data Engineering&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/Real-time-Problem-Statement/Problem-statement">Real time Problem Statement 🔋</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-1">The Art of Data Engineering:Crafting Robust and Scalable Solutions 🚀 </a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/SQL/sql-quick-review">Structured Query Language (SQL)</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/data-platforms/basics-of-a-data-platform">Data Platforms</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/data-sphere.github.io/docs/batch-processing/multi-hop-medallion-domain-questions">Batch Processing</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/multi-hop-medallion-domain-questions">The Multi-Hop/Medallion Architecture, Applied</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/exercise-bronze">Exercise: Bronze</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/apache-spark">Apache Spark</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/exercise-silver">Exercise: Silver</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/etl-vs-elt">ETL vs ELT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/data-testing">Data Testing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/learn-more-about-the-spark-ui">Want to learn more about the Spark UI? (Bonus)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/spark-workflow-and-partitioning">Spark Workflow and Partitioning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/exercise-gold-2">Exercise: Gold</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/exercise-workflow">Exercise: Create a Workflow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/exercise-query-engine">Exercise: Query your Data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/production-code-example">Production Code (Bonus)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/udfs">UDFs (Bonus)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/exercise-additional-spark-functions">Additional Spark Functions (Bonus)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/spark-advanced-topics">Spark Advanced Topics (Bonus)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/quiz">Quiz</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/data-quality/data_reliability">Data Quality [WIP]</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/data-visualisation/understanding-your-data">Data Visualisation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/beyond-the-batch/intro-to-streaming">Beyond the Batch</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/data-science-and-interpretability/data-science-data-requirements">Data Science and Interpretability</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/data-mesh/intro">Data Mesh 🕸</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/data-sphere.github.io/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Batch Processing</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Apache Spark</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Apache Spark</h1><p>Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python, and R, and an optimised engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing. </p><p><a href="https://spark.apache.org/docs/2.0.1/" target="_blank" rel="noopener noreferrer">Source: Apache Spark</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="apache-spark-topology">Apache Spark Topology<a href="#apache-spark-topology" class="hash-link" aria-label="Direct link to Apache Spark Topology" title="Direct link to Apache Spark Topology">​</a></h2><p>A typical spark cluster looks like below</p><div style="text-align:center"><p><img loading="lazy" alt="spark-cluster-topology.png" src="/data-sphere.github.io/assets/images/spark-cluster-topology-1ee8efbeb4a44896fc9b3c3651ccbaa8.png" width="512" height="246" class="img_ev3q"></p></div><p>There are several useful things to note about this architecture:</p><ol><li>Three entities on a typical Spark Cluster - Driver, Cluster Manager and Workers as shown above.</li><li>Upon submission of Spark jobs, each Spark Application gets its own executor processes (executors are like JVM runtimes), which stay up for the duration of the whole application and run tasks in multiple threads. This has the benefit of isolating applications from each other, on both the scheduling side (each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs). However, it also means that data cannot be shared across different Spark applications without writing it to an external storage system.</li><li>Spark is agnostic to the underlying cluster manager. If it can acquire executor processes which communicate with each other, it is relatively easy to run Spark Applications even on a cluster manager that also supports other applications (e.g. Mesos/YARN/Kubernetes).</li><li>The driver program must listen for and accept incoming connections from its executors throughout its lifetime. As such, the driver program must be network addressable from the worker nodes.</li><li>Because the driver schedules tasks on the cluster, it should be run close to the worker nodes, preferably on the same local area network. </li></ol><p><a href="https://spark.apache.org/docs/latest/cluster-overview.html" target="_blank" rel="noopener noreferrer">Source: Apache Spark</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-transformation-what-is-an-action">What is a transformation? What is an action?<a href="#what-is-a-transformation-what-is-an-action" class="hash-link" aria-label="Direct link to What is a transformation? What is an action?" title="Direct link to What is a transformation? What is an action?">​</a></h2><p>Spark offers two different kinds of operations:</p><ol><li><p><strong>Transformations</strong></p><p>Transformations are lazy operations which usually create or transform the data in one or the other way. For example:</p><p><strong>a) range (1,10)</strong> transformation creates a dataset of 10 variables ranging from 0 to 9.</p><p><strong>b) filter(x &gt; 10)</strong> will return all variables greater than 10 from the dataset by the name x.</p><p>Transformations themselves are of two types:</p><p><strong>a) Narrow Transformations:</strong> These transformations do not entail any network communication (e.g. range and filter transformations)</p><p><strong>b) Wide Transformations:</strong> These transformations do entail network communication (e.g. groupBy, joins, repartition). Wide Transformations usually result in redistributing data across partitions (also known as shuffling of data, explained in a later section). Redistribution is defining behaviour in wide transformations.For example, a groupBy redistributes data across partitions by the key on which grouping is needed. A join redistributes data across partitions to match joining keys.</p></li><li><p><strong>Actions</strong></p><p>Actions in Spark are eager operations that trigger the execution of logic. For example, a count action triggers a job to count the number of variables created or present in a dataset. A collect collects all the data from the partitions and returns some output. Actions are usually written towards the end of your code after all the transformations are completed.</p></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-are-transformations-and-actions-evaluated-in-spark-as-a-distributed-system">How are transformations and actions evaluated in Spark (as a distributed system)?<a href="#how-are-transformations-and-actions-evaluated-in-spark-as-a-distributed-system" class="hash-link" aria-label="Direct link to How are transformations and actions evaluated in Spark (as a distributed system)?" title="Direct link to How are transformations and actions evaluated in Spark (as a distributed system)?">​</a></h2><p>Transformations and Actions are evaluated differently in Spark which lends its processing efficiency. Transformations are lazy operations which usually create or transform the data and are not evaluated until an action operation is called. Let us take a very simple example of the below code to understand this better:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">spark.range(1,1000)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The above transformation creates a dataset. This dataset gets created on the worker machines as a number of input partition objects encapsulating the data. Input partitions are the in-memory JVM objects containing parts of the dataset that is being created here.  These input partition objects in turn are distributed across worker machines.
<a href="https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/demo-vanilla-spark#spark-input-partitions-stages-and-dags" target="_blank" rel="noopener noreferrer">Bonus: Learn more about input partitions here!</a></p><p>However, since Spark employs Lazy Loading, the partition objects do not get created in the memory of the worker machines unless an action is triggered. So let us add an action to the above code:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">spark.range(1,1000).count()</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>When this program is submitted on the driver machine of the Spark cluster, the following steps occur:</p><ol><li>The Spark Driver process with the help of Cluster Manager checks and identifies the worker on which this program will run</li><li>Once the workers are identified, the Driver process instructs the Worker Processes to spawn the executors.</li><li>Once the executors are up and running, the worker process initiates the creation of input partition objects encapsulating the data in each executor memory.</li><li>Since the actions is <code>count</code>, the counting of variables insides each partition happens next</li><li>The partition count totals are shuffled over (more on this in the later sections) to one of the identified work machines which does the overall count.</li><li>The overall count is sent back to the driver. The driver shows the output on the console.</li></ol><p><a href="https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/demo-vanilla-spark#spark-tasks-and-operations" target="_blank" rel="noopener noreferrer">Bonus: Learn more about Spark Tasks and Operations!</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-makes-spark-fast-and-relevant-to-our-data-work">What makes Spark fast and relevant to our data work?<a href="#what-makes-spark-fast-and-relevant-to-our-data-work" class="hash-link" aria-label="Direct link to What makes Spark fast and relevant to our data work?" title="Direct link to What makes Spark fast and relevant to our data work?">​</a></h2><p>Spark is fast and most relevant to the data work due to its following characteristics:</p><ol><li><p><strong>Lazily Loaded</strong></p><p>Lazy loading of Spark means that when you submit a job, Spark will only do its magic when it has to, specifically, when it receives an action (when the driver asks for some data or when it needs to write or store data; more details on this in a further section).</p></li><li><p><strong>DAG-based</strong></p><p>Instead of running the transformations one by one as soon as it receives them, Spark stores   these transformations in a DAG (Directed Acyclic Graph), and as soon as it receives an action, it runs the whole DAG and delivers the requested output. This enables it to optimise its execution based on the job’s DAG, instead of running the transformations sequentially one after the other. <a href="https://www.invivoo.com/en/why-is-spark-fast-and-how-to-make-it-run-faster-part-ii-the-spark-magic/" target="_blank" rel="noopener noreferrer">Source: Invivoo</a></p></li><li><p><strong>In Memory Computation with an optional Caching Component</strong></p><p>Spark does all its computations in memory by default. As mentioned above (in “How are transformations and actions evaluated in Spark (as a distributed system)?” section), the partitions containing  the data generated by transformations are pulled into  the memory of the worker machines at the trigger of an action. These partitions get garbage-collected as soon as the job is completed and are unavailable for subsequent jobs that require the same data. To avoid this, Spark gives the option of caching partitions. This further optimises the subsequent reruns of the same job as the partitions are in memory already due to caching. Learn more about it
<a href="https://data-derp.github.io/docs/2.0/data-engineering-the-good-parts/demo-vanilla-spark#spark-caching---introduction" target="_blank" rel="noopener noreferrer">here</a></p></li><li><p><strong>Framework for Distributed Computations</strong></p><p>Spark provides a framework for distributed in-memory computation. Such a facility is not  available in programming languages like Java, Python etc. One would argue that running a Java or a Python program on a distributed cluster framework like Kubernetes would effectively give the same result. Reality is, it does not due to the following reasons:</p><ol><li>Java/Python programming languages are not distributed in nature and are designed to run on single machines set up with a very small amount of data. Spark is designed to work in a distributed environment with huge amounts of data such that it distributes the data appropriately to optimise the job; this would be a manual effort in Kubernetes.</li><li>A Java/Python program in Kubernetes would not provide features mentioned above (lazy loading, DAGs, In Memory computations etc). In fact, should the need arise,  Spark already has an option of running it on Kubernetes as the distributed platform. This brings in all the features of Spark to the distributed characteristics of Kubernetes</li></ol></li></ol><p>This is completely separate to running <a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html" target="_blank" rel="noopener noreferrer">Spark on Kubernetes</a>, and at the time of writing, there is a common opinion that it is extremely difficult to manage and maintain, signalled by a large variety of SaaS solutions that exist for that offering. Of course, this is always up for debate. :)</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="additional-resources-bonus">Additional Resources (Bonus)<a href="#additional-resources-bonus" class="hash-link" aria-label="Direct link to Additional Resources (Bonus)" title="Direct link to Additional Resources (Bonus)">​</a></h2><ul><li><a href="https://www.youtube.com/embed/Ud6luYCkkMk" target="_blank" rel="noopener noreferrer">Apache Spark Introduction</a></li><li><a href="https://www.youtube.com/watch?v=p4PkA2huzVc" target="_blank" rel="noopener noreferrer">History of Spark</a> (first 3 minutes, skim the rest)</li><li><a href="https://brookewenig.com/SparkOverview.html#/" target="_blank" rel="noopener noreferrer">Overview of Spark (slides) by Brooke Wenig</a></li><li><a href="http://spark.apache.org/docs/3.1.1/sql-getting-started.html" target="_blank" rel="noopener noreferrer">Spark SQL Programming Guide (Python or Scala recommended)</a></li><li><a href="http://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html#dataframe-apis" target="_blank" rel="noopener noreferrer">DataFrame Methods</a> - e.g. df.someMethod()</li><li><a href="http://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html#column-apis" target="_blank" rel="noopener noreferrer">Column Methods</a> - e.g. F.col(&quot;myField&quot;).someMethod()</li><li><a href="http://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html#data-types" target="_blank" rel="noopener noreferrer">The different Data Types in Spark!</a></li><li><a href="http://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html#functions" target="_blank" rel="noopener noreferrer">The Sacred Texts</a> - pyspark.sql.functions</li><li><a href="http://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html#window" target="_blank" rel="noopener noreferrer">Windows</a> - you’re going to want to read this carefully 😉</li><li><a href="https://www.udemy.com/course/pyspark-aws-master-big-data-with-pyspark-and-aws/" target="_blank" rel="noopener noreferrer">Campus PySpark &amp; AWS + Spark Architectures</a> (optional, when you have time)</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/batch-processing/apache-spark.mdx" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/data-sphere.github.io/docs/batch-processing/exercise-bronze"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Exercise: Bronze</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/data-sphere.github.io/docs/batch-processing/exercise-silver"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Exercise: Silver</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#apache-spark-topology" class="table-of-contents__link toc-highlight">Apache Spark Topology</a></li><li><a href="#what-is-a-transformation-what-is-an-action" class="table-of-contents__link toc-highlight">What is a transformation? What is an action?</a></li><li><a href="#how-are-transformations-and-actions-evaluated-in-spark-as-a-distributed-system" class="table-of-contents__link toc-highlight">How are transformations and actions evaluated in Spark (as a distributed system)?</a></li><li><a href="#what-makes-spark-fast-and-relevant-to-our-data-work" class="table-of-contents__link toc-highlight">What makes Spark fast and relevant to our data work?</a></li><li><a href="#additional-resources-bonus" class="table-of-contents__link toc-highlight">Additional Resources (Bonus)</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/data-sphere.github.io/docs/tour_agenda">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Info</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Data-Dynamos/data-dynamos.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Data Dynamos</div></div></div></footer></div>
<script src="/data-sphere.github.io/assets/js/runtime~main.9b41e559.js"></script>
<script src="/data-sphere.github.io/assets/js/main.9f669193.js"></script>
</body>
</html>