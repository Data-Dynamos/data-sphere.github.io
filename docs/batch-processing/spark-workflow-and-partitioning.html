<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-batch-processing/spark-workflow-and-partitioning">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.0">
<title data-rh="true">Spark Workflow and Partitioning |  DataSphere</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://data-dynamos.github.io/data-sphere.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://data-dynamos.github.io/data-sphere.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://data-dynamos.github.io/data-sphere.github.io/docs/batch-processing/spark-workflow-and-partitioning"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Spark Workflow and Partitioning |  DataSphere"><meta data-rh="true" name="description" content="Optimisation"><meta data-rh="true" property="og:description" content="Optimisation"><link data-rh="true" rel="icon" href="/data-sphere.github.io/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://data-dynamos.github.io/data-sphere.github.io/docs/batch-processing/spark-workflow-and-partitioning"><link data-rh="true" rel="alternate" href="https://data-dynamos.github.io/data-sphere.github.io/docs/batch-processing/spark-workflow-and-partitioning" hreflang="en"><link data-rh="true" rel="alternate" href="https://data-dynamos.github.io/data-sphere.github.io/docs/batch-processing/spark-workflow-and-partitioning" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/data-sphere.github.io/blog/rss.xml" title=" DataSphere RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/data-sphere.github.io/blog/atom.xml" title=" DataSphere Atom Feed"><link rel="stylesheet" href="/data-sphere.github.io/assets/css/styles.5e85331d.css">
<link rel="preload" href="/data-sphere.github.io/assets/js/runtime~main.9b41e559.js" as="script">
<link rel="preload" href="/data-sphere.github.io/assets/js/main.9f669193.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/data-sphere.github.io/"><div class="navbar__logo"><img src="/data-sphere.github.io/img/datasphere_1.jpeg" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/data-sphere.github.io/img/datasphere_1.jpeg" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">DataSphere</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/data-sphere.github.io/docs/tour_agenda">Lessons</a><a class="navbar__item navbar__link" href="/data-sphere.github.io/resources">Resources</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/Data-Dynamos/data-dynamos.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/data-sphere.github.io/docs/tour_agenda">Tour Agenda</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/data-sphere.github.io/docs/category/intro-to-data-engineering">Intro to Data Engineering</a><button aria-label="Toggle the collapsible sidebar category &#x27;Intro to Data Engineering&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/Real-time-Problem-Statement/Problem-statement">Real time Problem Statement 🔋</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-1">The Art of Data Engineering:Crafting Robust and Scalable Solutions 🚀 </a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/SQL/sql-quick-review">Structured Query Language (SQL)</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/data-platforms/basics-of-a-data-platform">Data Platforms</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/data-sphere.github.io/docs/batch-processing/multi-hop-medallion-domain-questions">Batch Processing</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/multi-hop-medallion-domain-questions">The Multi-Hop/Medallion Architecture, Applied</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/exercise-bronze">Exercise: Bronze</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/apache-spark">Apache Spark</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/exercise-silver">Exercise: Silver</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/etl-vs-elt">ETL vs ELT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/data-testing">Data Testing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/learn-more-about-the-spark-ui">Want to learn more about the Spark UI? (Bonus)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/spark-workflow-and-partitioning">Spark Workflow and Partitioning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/exercise-gold-2">Exercise: Gold</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/exercise-workflow">Exercise: Create a Workflow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/exercise-query-engine">Exercise: Query your Data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/production-code-example">Production Code (Bonus)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/udfs">UDFs (Bonus)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/exercise-additional-spark-functions">Additional Spark Functions (Bonus)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/spark-advanced-topics">Spark Advanced Topics (Bonus)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data-sphere.github.io/docs/batch-processing/quiz">Quiz</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/data-quality/data_reliability">Data Quality [WIP]</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/data-visualisation/understanding-your-data">Data Visualisation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/beyond-the-batch/intro-to-streaming">Beyond the Batch</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/data-science-and-interpretability/data-science-data-requirements">Data Science and Interpretability</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data-sphere.github.io/docs/data-mesh/intro">Data Mesh 🕸</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/data-sphere.github.io/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Batch Processing</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Spark Workflow and Partitioning</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Spark Workflow and Partitioning</h1><div style="text-align:center"><figure class="video-container"><iframe width="560" height="315" src="https://www.youtube.com/embed/5HjbnUEBE5g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe></figure></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="optimisation">Optimisation<a href="#optimisation" class="hash-link" aria-label="Direct link to Optimisation" title="Direct link to Optimisation">​</a></h2><p>At this point, we&#x27;ve wrangled/transformed our data...but how do we actually optimize our job’s performance?
<strong>Golden Rule:</strong> In the real world, make sure your dataset/table is partitioned well.</p><ul><li>Lots of small files are the enemy!<ul><li>Having lots of tiny files will result in S3 needing to do lots of <strong>file listing</strong> operations. These are extremely slow and can even be expensive</li><li>Lots of small files means lots of data shuffling through the network. <strong>This is slow!</strong></li></ul></li><li><strong>HUGE</strong> files are also bad<ul><li>Having too few files (all being huge) means you probably won’t take advantage of all of the cores in your cluster. In other words, the data can’t be easily distributed around the cluster</li><li>Each node in your cluster might even have to try and break down each of these huge files in order to redistribute some data to other nodes. This is a waste of time and money (<a href="https://www.youtube.com/watch?v=982wFqC03v8&amp;ab_channel=pyromaniack" target="_blank" rel="noopener noreferrer">to emphasise the point</a>). </li></ul></li></ul><p>So what’s a suitable strategy?</p><ul><li>There’s no &quot;best&quot; number. In parquet, try to target each <code>.snappy.parquet</code> file to be somewhere <strong>roughly between 256MB to 1GB</strong></li><li>More importantly, make sure that you’re partitioning on columns that you frequently <strong>filter</strong> or do <strong>groupBy</strong> on (another reason to be Product-minded and ask your consumers what kinds of questions they&#x27;ll need answered by your data)</li><li><strong>DO NOT</strong> partition on columns with high cardinality (e.g. a userId, which has millions of distinct values)
this will result in lots of <strong>small files and lots of file listing operations</strong></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="partitioning">Partitioning<a href="#partitioning" class="hash-link" aria-label="Direct link to Partitioning" title="Direct link to Partitioning">​</a></h2><div style="text-align:center"><p><img loading="lazy" alt="partitioning.png" src="/data-sphere.github.io/assets/images/partitioning-ac6b6c56d48894919e4f9058a0a8ab9c.png" width="379" height="477" class="img_ev3q"></p></div><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_S0QG"><p>Partitioning strategy is the most important decision we have to get right!</p></div></div><p>If your partitioning strategy is decent, you’ll most likely be fine and won’t need to tweak other knobs.
Especially going forward in the future with Spark 3.0’s <a href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html" target="_blank" rel="noopener noreferrer">Adaptive Query Execution (AQE)</a>, a lot of optimizations will be automated for you!</p><p>So how does a partitioned table look?</p><ul><li>It would actually look like a bunch of hierarchical folders</li><li>The partitioning values become their own folder (e.g. year=2018)</li><li>The underlying data will be at the bottom of the hierarchy and will</li><li>often have a <code>.snappy.parquet</code> file extension (if using Spark and Parquet)</li></ul><p>Can you give me an example?</p><ul><li>Partitioning the table based on some notion of time is a popular option
(check if that makes sense for your consumers and your use case though!)<ul><li>e.g. assuming each day of data for the table is of the order of 128MB - 1GB, then
your partitioning keys can be (“year”, “month”, “day”)</li></ul></li><li>You don’t need to explicitly define all the values, Spark will smartly
create a new partition for each distinct combination of your partitioning values</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="working-with-partitioned-data">Working with Partitioned Data<a href="#working-with-partitioned-data" class="hash-link" aria-label="Direct link to Working with Partitioned Data" title="Direct link to Working with Partitioned Data">​</a></h2><div style="text-align:center"><figure class="video-container"><iframe width="560" height="315" src="https://www.youtube.com/embed/fhEJG2oFCm8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe></figure></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="partitioning-faqs">Partitioning FAQs<a href="#partitioning-faqs" class="hash-link" aria-label="Direct link to Partitioning FAQs" title="Direct link to Partitioning FAQs">​</a></h2><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>So...is a parquet file a file or a folder of files?</summary><div><div class="collapsibleContent_i85q">Either!</div></div></details><ul><li>With a single-node library like <a href="https://pandas.pydata.org/" target="_blank" rel="noopener noreferrer">Pandas</a>, you can write a single <code>.snappy.parquet</code> file if you want</li><li>In the real-world they are often folders of partitions<ul><li>This way you can read/write an entire table with just one path (the root of the table)<ul><li>e.g. <code>s3://my-bucket/my-table/</code> or <code>s3://my-bucket/my-table.parquet/</code> (both of these styles are still folders)</li><li>Underneath all the partitioning folders, you will find <code>.snappy.parquet</code> files</li></ul></li><li>The query engine (e.g. Spark or Presto) will then take care of understanding the partitioning structure of the table and will optimize your queries around that</li><li>Spark will always write the output of a DataFrame as a folder at the root level rather than a single file (because it’s designed for distributed/concurrent reading/writing of data, which often involves multiple files)</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="shuffling">Shuffling<a href="#shuffling" class="hash-link" aria-label="Direct link to Shuffling" title="Direct link to Shuffling">​</a></h2><div style="text-align:center"><p><img loading="lazy" alt="shuffling.png" src="/data-sphere.github.io/assets/images/shuffling-ed3cfadd55c02ea6dddadb4cb0f42f96.png" width="424" height="512" class="img_ev3q"></p><p><a href="https://blog.scottlogic.com/2018/03/22/apache-spark-performance.html#:~:text=A%20shuffle%20occurs%20when%20data,likely%20on%20a%20different%20executor." target="_blank" rel="noopener noreferrer">Reference</a></p></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="spark-operations">Spark Operations<a href="#spark-operations" class="hash-link" aria-label="Direct link to Spark Operations" title="Direct link to Spark Operations">​</a></h3><p>There are 2 kind of operations available in Apache Spark</p><ol><li><p><strong>Transformations</strong></p><p>Transformations are lazy operations which usually create or transform the data in one or other way. For example:</p><p>a) <strong>range (1,10)</strong> transformation creates a dataset of 10 variables ranging from 0 to 9.</p><p>b) <strong>filter(x &gt; 10)</strong> will return all variables greater than 10 from the dataset by the name x.</p><p>Transformations themselves are of two types:</p><p>a) <strong>Narrow Transformations</strong>: These transformations do not entail any network  communication (e.g. range and filter transformations)</p><p>b) <strong>Wide Transformations</strong>: These transformations do entail network communication (e.g. groupBy, joins, repartition). Wide Transformations usually result in redistributing data across partitions (also known as shuffling of data, section below)). Redistribution is defining behaviour in wide transformations.For example,  a <code>groupBy</code> redistributes  data across partitions by the key on which grouping is needed. A <code>join</code> redistributes data across partitions to match joining keys.</p></li><li><p><strong>Actions</strong></p><p>Actions in Spark are eager operations that trigger the execution of logic. For example, a <code>count</code> action triggers a job to count the number of variable created or present in a dataset. A <code>collect</code> collects all the data from the partitions and returns some output. Actions are usually written towards the end of your code after all the transformations are completed.</p></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="shuffles-in-spark">Shuffles in Spark<a href="#shuffles-in-spark" class="hash-link" aria-label="Direct link to Shuffles in Spark" title="Direct link to Shuffles in Spark">​</a></h3><p>Wide operations within Spark trigger an event known as the shuffle. The shuffle is Spark’s mechanism for redistribution. This typically involves copying data across executors and machines, making the shuffle a complex and costly operation and should be avoided as much as possible, unless it is absolutely necessary.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="operations-that-create-a-shuffle">Operations that create a shuffle<a href="#operations-that-create-a-shuffle" class="hash-link" aria-label="Direct link to Operations that create a shuffle" title="Direct link to Operations that create a shuffle">​</a></h3><p>Shuffles in Spark usually come into the picture when a wide transformation (like groupBy, joins, repartition etc) is used in transformation logic. Let us take the example of a <code>groupBy</code> transformation to understand this better. Spark programs usually read the data in the form of in-memory partitions. The data in these partitions is distributed across different machines on the cluster and is by default not grouped on a particular field. If we apply <code>groupBy</code> on a certain field like <code>country</code> for example, the Spark runtime will have to shuffle the data across the initial in-memory partitions to make sure that all the data from the same country  moves to the same partition.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="acceptable-shuffles-and-avoiding-shuffles">Acceptable Shuffles and Avoiding Shuffles<a href="#acceptable-shuffles-and-avoiding-shuffles" class="hash-link" aria-label="Direct link to Acceptable Shuffles and Avoiding Shuffles" title="Direct link to Acceptable Shuffles and Avoiding Shuffles">​</a></h3><p>As you might have guessed by now, shuffling is a necessary evil, especially for wide transformations in Spark. A data shuffle happens if we are using transformations like groupBy, joins and repartition. Since shuffles are expensive, the best way to avoid them would be to:</p><ol><li>write your logic  using narrow transformations like map, flatmaps and filter.</li><li>use wide transformations only when required and no other alternative is available in narrow transformations.</li><li>use wide transformations as late as possible or towards the end of your logic (e.g.join only at the end of your logic)</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="resources-bonus">Resources (Bonus)<a href="#resources-bonus" class="hash-link" aria-label="Direct link to Resources (Bonus)" title="Direct link to Resources (Bonus)">​</a></h2><div style="text-align:center"><p><img loading="lazy" alt="fear-path-to-dark-side.png" src="/data-sphere.github.io/assets/images/fear-path-to-dark-side-02df5aba18d3a6b7209b559ad8883194.png" width="512" height="288" class="img_ev3q"></p></div><ul><li><a href="https://blog.rockthejvm.com/spark-dags/" target="_blank" rel="noopener noreferrer">Spark DAGs and planning</a> (optional)<ul><li>Just know that bad partitioning → shuffling → pain (must-watch)</li><li>You can check how ‘shuffly’ your Spark job looks by viewing the DAG</li></ul></li><li><a href="https://mungingdata.com/apache-spark/partitionby/" target="_blank" rel="noopener noreferrer">Managing Partitioning</a><ul><li>Important: understand that repartition() and DataFrame.write.partitionBy() are <strong>not</strong> the same thing<ul><li>Repartition can take in 2 different types of arguments:<ul><li>a number: controls the number of .snappy.parquet files</li><li>a bunch of column names: it will ensure 1 .snappy.parquet file per each distinct combination of your provided columns</li></ul></li><li>DataFrame.write.partitionBy defines the folder structure of the table <ul><li>however, it does not guarantee how many .snappy.parquet files will be in each folder </li></ul></li><li>Sometimes you might even need to do both e.g. <code>df.repartition(“year”, “month”).write.partitionBy(“year”, ”month”)...</code> in order to guarantee exactly 1 .snappy.parquet file per each month folder</li></ul></li><li>Try to read up on the difference between repartition and coalesce <ul><li>Short Answer: <a href="https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.coalesce" target="_blank" rel="noopener noreferrer">The coalesce transformation applied to a DataFrame</a> (not to be confused with <a href="https://spark.apache.org/docs/latest/api/python//reference/pyspark.sql/api/pyspark.sql.functions.coalesce.html" target="_blank" rel="noopener noreferrer">coalesce() applied to a column</a>), will try to merge partitions to reach your desired number. You only use coalesce when you want to reduce the number of partitions in your data.</li><li>On the other hand, repartition() will full shuffle all of the data around (more expensive).</li><li>If you need to increase the number of partitions in your data, then you will need repartition()</li></ul></li></ul></li></ul><p><strong>Note</strong> - If not done already, it will be a good idea to revise the repartitioning and partition by videos from <a href="/data-sphere.github.io/docs/data-engineering-the-good-parts/011-demo-vanilla-spark.md">here</a> before attempting the below exercise</p><p>Follow the instructions <a href="https://github.com/data-derp/small-exercises/tree/master/databricks-repartition-vs-write-partition-by#readme" target="_blank" rel="noopener noreferrer">here</a> to import the repartitioning exercise notebook</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/batch-processing/spark-workflow-and-partitioning.mdx" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/data-sphere.github.io/docs/batch-processing/learn-more-about-the-spark-ui"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Want to learn more about the Spark UI? (Bonus)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/data-sphere.github.io/docs/batch-processing/exercise-gold-2"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Exercise: Gold</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#optimisation" class="table-of-contents__link toc-highlight">Optimisation</a></li><li><a href="#partitioning" class="table-of-contents__link toc-highlight">Partitioning</a></li><li><a href="#working-with-partitioned-data" class="table-of-contents__link toc-highlight">Working with Partitioned Data</a></li><li><a href="#partitioning-faqs" class="table-of-contents__link toc-highlight">Partitioning FAQs</a></li><li><a href="#shuffling" class="table-of-contents__link toc-highlight">Shuffling</a><ul><li><a href="#spark-operations" class="table-of-contents__link toc-highlight">Spark Operations</a></li><li><a href="#shuffles-in-spark" class="table-of-contents__link toc-highlight">Shuffles in Spark</a></li><li><a href="#operations-that-create-a-shuffle" class="table-of-contents__link toc-highlight">Operations that create a shuffle</a></li><li><a href="#acceptable-shuffles-and-avoiding-shuffles" class="table-of-contents__link toc-highlight">Acceptable Shuffles and Avoiding Shuffles</a></li></ul></li><li><a href="#resources-bonus" class="table-of-contents__link toc-highlight">Resources (Bonus)</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/data-sphere.github.io/docs/tour_agenda">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Info</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Data-Dynamos/data-dynamos.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Data Dynamos</div></div></div></footer></div>
<script src="/data-sphere.github.io/assets/js/runtime~main.9b41e559.js"></script>
<script src="/data-sphere.github.io/assets/js/main.9f669193.js"></script>
</body>
</html>