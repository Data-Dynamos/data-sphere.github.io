"use strict";(self.webpackChunkdata_dynamos_github_io=self.webpackChunkdata_dynamos_github_io||[]).push([[1736],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>h});var r=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},o=Object.keys(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var c=r.createContext({}),d=function(e){var t=r.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},p=function(e){var t=d(e.components);return r.createElement(c.Provider,{value:t},e.children)},l="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,o=e.originalType,c=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),l=d(a),u=n,h=l["".concat(c,".").concat(u)]||l[u]||m[u]||o;return a?r.createElement(h,s(s({ref:t},p),{},{components:a})):r.createElement(h,s({ref:t},p))}));function h(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=a.length,s=new Array(o);s[0]=u;var i={};for(var c in t)hasOwnProperty.call(t,c)&&(i[c]=t[c]);i.originalType=e,i[l]="string"==typeof e?e:n,s[1]=i;for(var d=2;d<o;d++)s[d]=a[d];return r.createElement.apply(null,s)}return r.createElement.apply(null,a)}u.displayName="MDXCreateElement"},9529:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>d});var r=a(7462),n=(a(7294),a(3905));const o={sidebar_position:10,minutesToComplete:5,authors:["syed","kmok"]},s="Sharing Streaming Data",i={unversionedId:"beyond-the-batch/sharing-streaming-data",id:"beyond-the-batch/sharing-streaming-data",title:"Sharing Streaming Data",description:"In the prior examples, we've displayed our resulting aggregated stream data in the console. In the scenario where we would like to share the results of our aggregated streaming data with others, we can write that to a remote data store in the form of csv, json, parquet, and other formats similar to writing Batch data to remote data stores (Sink). It is also possible to write directly to Transactional databases, such as Cassandra or DynamoDB or even back to a Kafka topic. Spark supports writes to certain remote data stores out of the box (e.g. Cassandra) and some require leveraging the foreach operator plus some custom code to interact directly with the remote data store.",source:"@site/docs/beyond-the-batch/sharing-streaming-data.mdx",sourceDirName:"beyond-the-batch",slug:"/beyond-the-batch/sharing-streaming-data",permalink:"/docs/beyond-the-batch/sharing-streaming-data",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/beyond-the-batch/sharing-streaming-data.mdx",tags:[],version:"current",sidebarPosition:10,frontMatter:{sidebar_position:10,minutesToComplete:5,authors:["syed","kmok"]},sidebar:"tutorialSidebar",previous:{title:"Stateful Streaming in a Nutshell",permalink:"/docs/beyond-the-batch/stateful-streaming-nutshell"},next:{title:"Exercise: Streaming",permalink:"/docs/beyond-the-batch/exercise-streaming"}},c={},d=[{value:"Writing to Cassandra",id:"writing-to-cassandra",level:2},{value:"Writing to DynamoDB",id:"writing-to-dynamodb",level:2}],p={toc:d},l="wrapper";function m(e){let{components:t,...a}=e;return(0,n.kt)(l,(0,r.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"sharing-streaming-data"},"Sharing Streaming Data"),(0,n.kt)("p",null,"In the prior examples, we've displayed our resulting aggregated stream data in the console. In the scenario where we would like to share the results of our aggregated streaming data with others, we can write that to a remote data store in the form of csv, json, parquet, and other formats similar to writing Batch data to remote data stores (Sink). It is also possible to write directly to Transactional databases, such as ",(0,n.kt)("a",{parentName:"p",href:"https://docs.databricks.com/structured-streaming/examples.html#write-to-cassandra-as-a-sink-for-structured-streaming-in-python"},"Cassandra")," or ",(0,n.kt)("a",{parentName:"p",href:"https://docs.databricks.com/structured-streaming/examples.html#write-to-amazon-dynamodb-using-foreach-in-scala-and-python"},"DynamoDB")," or even back to a Kafka topic. Spark supports writes to certain remote data stores out of the box (e.g. Cassandra) and some require leveraging the ",(0,n.kt)("inlineCode",{parentName:"p"},"foreach")," operator plus some custom code to interact directly with the remote data store."),(0,n.kt)("h2",{id:"writing-to-cassandra"},"Writing to Cassandra"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'spark.conf.set("spark.cassandra.connection.host", "host1,host2")\n\ndf.writeStream \\\n  .format("org.apache.spark.sql.cassandra") \\\n  .outputMode("append") \\\n  .option("checkpointLocation", "/path/to/checkpoint") \\\n  .option("keyspace", "keyspace_name") \\\n  .option("table", "table_name") \\\n  .start()\n')),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://docs.databricks.com/structured-streaming/examples.html#write-to-cassandra-as-a-sink-for-structured-streaming-in-python"},"Source")),(0,n.kt)("h2",{id:"writing-to-dynamodb"},"Writing to DynamoDB"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'from pyspark.sql.functions import *\n\nspark.conf.set("spark.sql.shuffle.partitions", "1")\n\nquery = (\n  spark.readStream.format("rate").load()\n    .selectExpr("value % 10 as key")\n    .groupBy("key")\n    .count()\n    .toDF("key", "count")\n    .writeStream\n    .foreach(SendToDynamoDB_ForeachWriter())  # => custom code. See https://docs.databricks.com/structured-streaming/examples.html#write-to-amazon-dynamodb-using-foreach-in-scala-and-python for details.\n    .outputMode("update")\n    .start()\n)\n')),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://docs.databricks.com/structured-streaming/examples.html#write-to-amazon-dynamodb-using-foreach-in-scala-and-python"},"Source")))}m.isMDXComponent=!0}}]);