"use strict";(self.webpackChunkdata_dynamos_github_io=self.webpackChunkdata_dynamos_github_io||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Tour Agenda","href":"/data-sphere.github.io/docs/tour_agenda/","docId":"tour_agenda/tour_agenda"},{"type":"category","label":"Intro to Data Engineering","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"What is Data Engineering?","href":"/data-sphere.github.io/docs/Intro-to-Data-Engineering/what-is-data-engineering","docId":"Intro-to-Data-Engineering/what-is-data-engineering"},{"type":"link","label":"Data Science vs. Data Engineer vs. Data Analyst","href":"/data-sphere.github.io/docs/Intro-to-Data-Engineering/data-scientist-engineer-analyst","docId":"Intro-to-Data-Engineering/data-scientist-engineer-analyst"},{"type":"link","label":"The Importance of Data Democratisation","href":"/data-sphere.github.io/docs/Intro-to-Data-Engineering/data-democratisation","docId":"Intro-to-Data-Engineering/data-democratisation"}],"href":"/data-sphere.github.io/docs/category/intro-to-data-engineering"},{"type":"category","label":"The Art of Data Engineering:Crafting Robust and Scalable Solutions \ud83d\ude80 ","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Data Milky Way: A Brief History (Part 1) - OLTP vs. OLAP","href":"/data-sphere.github.io/docs/The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-1","docId":"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-1"},{"type":"link","label":"Data Milky Way: Distributed Data Systems (NoSQL & CAP)","href":"/data-sphere.github.io/docs/The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-nosql-cap-theorem","docId":"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-nosql-cap-theorem"},{"type":"link","label":"Data Milky Way: A Brief History (Part 2) - Evolution","href":"/data-sphere.github.io/docs/The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-2-evolution","docId":"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-2-evolution"},{"type":"link","label":"Data Milky Way: Brief History (Part 3) - Data Processing","href":"/data-sphere.github.io/docs/The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-3-data-processing","docId":"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-3-data-processing"},{"type":"link","label":"Data Milky Way: A Brief History (Part 4) - Architecture Reference","href":"/data-sphere.github.io/docs/The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-4-arch-ref","docId":"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-4-arch-ref"},{"type":"link","label":"Data Formats","href":"/data-sphere.github.io/docs/The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-formats","docId":"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-formats"}]},{"type":"category","label":"Real time Problem Statement \ud83d\udd0b","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Problem Statement \ud83d\udca1","href":"/data-sphere.github.io/docs/Real-time-Problem-Statement/Problem-statement","docId":"Real-time-Problem-Statement/Problem-statement"},{"type":"link","label":"Exercise: Last Connection Time of Charge Points","href":"/data-sphere.github.io/docs/Real-time-Problem-Statement/exercise-last-connection-charge-points","docId":"Real-time-Problem-Statement/exercise-last-connection-charge-points"},{"type":"link","label":"Exercise: Final Charge Time and Charge Dispense for Completed Charges","href":"/data-sphere.github.io/docs/Real-time-Problem-Statement/exercise-final-charge-time-dispense","docId":"Real-time-Problem-Statement/exercise-final-charge-time-dispense"},{"type":"link","label":"Reflect","href":"/data-sphere.github.io/docs/Real-time-Problem-Statement/reflect","docId":"Real-time-Problem-Statement/reflect"}]},{"type":"category","label":"Structured Query Language (SQL)","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"SQL: A Quick Review","href":"/data-sphere.github.io/docs/SQL/sql-quick-review","docId":"SQL/sql-quick-review"},{"type":"link","label":"Advanced SQL: Window and Aggregate Functions","href":"/data-sphere.github.io/docs/SQL/advance-sql","docId":"SQL/advance-sql"}]},{"type":"category","label":"Data Platforms","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Basics of a Data Platform","href":"/data-sphere.github.io/docs/data-platforms/basics-of-a-data-platform","docId":"data-platforms/basics-of-a-data-platform"},{"type":"link","label":"The Elements of a Data Product","href":"/data-sphere.github.io/docs/data-platforms/the-elements-of-a-data-product","docId":"data-platforms/the-elements-of-a-data-product"},{"type":"link","label":"Common Landmarks of Data Processing","href":"/data-sphere.github.io/docs/data-platforms/landmarks-of-data-processing","docId":"data-platforms/landmarks-of-data-processing"},{"type":"link","label":"Batch vs. Streaming","href":"/data-sphere.github.io/docs/data-platforms/batch-vs-streaming","docId":"data-platforms/batch-vs-streaming"}]},{"type":"category","label":"Batch Processing","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"The Multi-Hop/Medallion Architecture, Applied","href":"/data-sphere.github.io/docs/batch-processing/multi-hop-medallion-domain-questions","docId":"batch-processing/multi-hop-medallion-domain-questions"},{"type":"link","label":"Exercise: Bronze","href":"/data-sphere.github.io/docs/batch-processing/exercise-bronze","docId":"batch-processing/exercise-bronze"},{"type":"link","label":"Apache Spark","href":"/data-sphere.github.io/docs/batch-processing/apache-spark","docId":"batch-processing/apache-spark"},{"type":"link","label":"Exercise: Silver","href":"/data-sphere.github.io/docs/batch-processing/exercise-silver","docId":"batch-processing/exercise-silver"},{"type":"link","label":"ETL vs ELT","href":"/data-sphere.github.io/docs/batch-processing/etl-vs-elt","docId":"batch-processing/etl-vs-elt"},{"type":"link","label":"Data Testing","href":"/data-sphere.github.io/docs/batch-processing/data-testing","docId":"batch-processing/data-testing"},{"type":"link","label":"Want to learn more about the Spark UI? (Bonus)","href":"/data-sphere.github.io/docs/batch-processing/learn-more-about-the-spark-ui","docId":"batch-processing/learn-more-about-the-spark-ui"},{"type":"link","label":"Spark Workflow and Partitioning","href":"/data-sphere.github.io/docs/batch-processing/spark-workflow-and-partitioning","docId":"batch-processing/spark-workflow-and-partitioning"},{"type":"link","label":"Exercise: Gold","href":"/data-sphere.github.io/docs/batch-processing/exercise-gold-2","docId":"batch-processing/exercise-gold-2"},{"type":"link","label":"Exercise: Create a Workflow","href":"/data-sphere.github.io/docs/batch-processing/exercise-workflow","docId":"batch-processing/exercise-workflow"},{"type":"link","label":"Exercise: Query your Data","href":"/data-sphere.github.io/docs/batch-processing/exercise-query-engine","docId":"batch-processing/exercise-query-engine"},{"type":"link","label":"Production Code (Bonus)","href":"/data-sphere.github.io/docs/batch-processing/production-code-example","docId":"batch-processing/production-code-example"},{"type":"link","label":"UDFs (Bonus)","href":"/data-sphere.github.io/docs/batch-processing/udfs","docId":"batch-processing/udfs"},{"type":"link","label":"Additional Spark Functions (Bonus)","href":"/data-sphere.github.io/docs/batch-processing/exercise-additional-spark-functions","docId":"batch-processing/exercise-additional-spark-functions"},{"type":"link","label":"Spark Advanced Topics (Bonus)","href":"/data-sphere.github.io/docs/batch-processing/spark-advanced-topics","docId":"batch-processing/spark-advanced-topics"},{"type":"link","label":"Quiz","href":"/data-sphere.github.io/docs/batch-processing/quiz","docId":"batch-processing/quiz"}]},{"type":"category","label":"Data Quality","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Data Reliability","href":"/data-sphere.github.io/docs/data-quality/data_reliability","docId":"data-quality/data_reliability"},{"type":"link","label":"Data Quality","href":"/data-sphere.github.io/docs/data-quality/dq_overwiew","docId":"data-quality/dq_overwiew"}]},{"type":"category","label":"Data Visualisation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Understanding Your Data","href":"/data-sphere.github.io/docs/data-visualisation/understanding-your-data","docId":"data-visualisation/understanding-your-data"},{"type":"link","label":"Visualization Tools","href":"/data-sphere.github.io/docs/data-visualisation/visualization-tools","docId":"data-visualisation/visualization-tools"},{"type":"link","label":"Exercise","href":"/data-sphere.github.io/docs/data-visualisation/exercise","docId":"data-visualisation/exercise"}]},{"type":"category","label":"Beyond the Batch","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Intro to Streaming","href":"/data-sphere.github.io/docs/beyond-the-batch/intro-to-streaming","docId":"beyond-the-batch/intro-to-streaming"},{"type":"link","label":"Spark Structured Streaming","href":"/data-sphere.github.io/docs/beyond-the-batch/spark-structured-streaming","docId":"beyond-the-batch/spark-structured-streaming"},{"type":"link","label":"Stateful vs. Stateless Streaming","href":"/data-sphere.github.io/docs/beyond-the-batch/stateful-vs-stateless-streaming","docId":"beyond-the-batch/stateful-vs-stateless-streaming"},{"type":"link","label":"Checkpointing","href":"/data-sphere.github.io/docs/beyond-the-batch/checkpointing","docId":"beyond-the-batch/checkpointing"},{"type":"link","label":"Handling Late Data","href":"/data-sphere.github.io/docs/beyond-the-batch/handling-late-data","docId":"beyond-the-batch/handling-late-data"},{"type":"link","label":"Stateful Streaming in a Nutshell","href":"/data-sphere.github.io/docs/beyond-the-batch/stateful-streaming-nutshell","docId":"beyond-the-batch/stateful-streaming-nutshell"},{"type":"link","label":"Sharing Streaming Data","href":"/data-sphere.github.io/docs/beyond-the-batch/sharing-streaming-data","docId":"beyond-the-batch/sharing-streaming-data"},{"type":"link","label":"Exercise: Streaming","href":"/data-sphere.github.io/docs/beyond-the-batch/exercise-streaming","docId":"beyond-the-batch/exercise-streaming"},{"type":"link","label":"Driving Streaming in Production","href":"/data-sphere.github.io/docs/beyond-the-batch/streaming-technologies","docId":"beyond-the-batch/streaming-technologies"}]},{"type":"category","label":"Data Science and Interpretability","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Data Science Data Requirements","href":"/data-sphere.github.io/docs/data-science-and-interpretability/data-science-data-requirements","docId":"data-science-and-interpretability/data-science-data-requirements"},{"type":"link","label":"Jump into Data Science (Bonus)","href":"/data-sphere.github.io/docs/data-science-and-interpretability/overview","docId":"data-science-and-interpretability/overview"},{"type":"link","label":"Want to learn more? (Bonus)","href":"/data-sphere.github.io/docs/data-science-and-interpretability/want-to-learn-more","docId":"data-science-and-interpretability/want-to-learn-more"}]},{"type":"category","label":"Data Mesh \ud83d\udd78","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction","href":"/data-sphere.github.io/docs/data-mesh/intro","docId":"data-mesh/intro"},{"type":"link","label":"Journey to Data Mesh (Optional)","href":"/data-sphere.github.io/docs/data-mesh/journey-to-data-mesh","docId":"data-mesh/journey-to-data-mesh"},{"type":"link","label":"What is Data Mesh? (Optional)","href":"/data-sphere.github.io/docs/data-mesh/overview","docId":"data-mesh/overview"},{"type":"link","label":"The analytical and operational planes (Optional)","href":"/data-sphere.github.io/docs/data-mesh/types-of-data","docId":"data-mesh/types-of-data"},{"type":"link","label":"Want to learn more? (Optional)","href":"/data-sphere.github.io/docs/data-mesh/resources","docId":"data-mesh/resources"}]}]},"docs":{"batch-processing/apache-spark":{"id":"batch-processing/apache-spark","title":"Apache Spark","description":"Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python, and R, and an optimised engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.","sidebar":"tutorialSidebar"},"batch-processing/data-testing":{"id":"batch-processing/data-testing","title":"Data Testing","description":"By now, you might have been acutely aware that there were a number of tests used throughout the previous exercises. They took the shape of either a (1) unit test or (2) an E2E (end-to-end) test. Just like traditional software development, there is a time and place to use either of those tests and it increases the readability and accountability of the transformations and additionally aids your colleagues (or yourself!) in future months to understand what had been implemented in the past (and ensures that existing functionality is not broken).","sidebar":"tutorialSidebar"},"batch-processing/etl-vs-elt":{"id":"batch-processing/etl-vs-elt","title":"ETL vs ELT","description":"Let\'s Be Pragmatic","sidebar":"tutorialSidebar"},"batch-processing/exercise-additional-spark-functions":{"id":"batch-processing/exercise-additional-spark-functions","title":"Additional Spark Functions (Bonus)","description":"There are quite a few more Spark functions that haven\'t been covered in the previous exercises. Have a quick browse (this is more of an FYI), just to get familiarised with it:","sidebar":"tutorialSidebar"},"batch-processing/exercise-bronze":{"id":"batch-processing/exercise-bronze","title":"Exercise: Bronze","description":"When creating long-term storage for analytical use cases, the first step is to ingest data for the source, with a shape as close as possible to the original shape. As the first step in our data processing journey, this allows us to","sidebar":"tutorialSidebar"},"batch-processing/exercise-gold-2":{"id":"batch-processing/exercise-gold-2","title":"Exercise: Gold","description":"We\'ve previously done some standardisation of our data, namely flattening and casting some columns to specific types, in the Silver exercise. In this exercise, we will take our beautifully standardised data and transform it using some business logic. Remember that wonderful Windows exercise? Get ready, because, (cracks knuckles) practice makes perfect!","sidebar":"tutorialSidebar"},"batch-processing/exercise-query-engine":{"id":"batch-processing/exercise-query-engine","title":"Exercise: Query your Data","description":"We previously talked a little bit about Query Engines.Query Engines  allow us to query our Data Lakes / unstructured data. There are many Query Engines currently in the market (at the time of writing) including AWS Athena, Presto, Trino, Dremio, Databricks SQL.","sidebar":"tutorialSidebar"},"batch-processing/exercise-silver":{"id":"batch-processing/exercise-silver","title":"Exercise: Silver","description":"In the last exercise, we ingested our data. In this exercise, we\'ll take our first step towards curation and cleanup by:","sidebar":"tutorialSidebar"},"batch-processing/exercise-workflow":{"id":"batch-processing/exercise-workflow","title":"Exercise: Create a Workflow","description":"Quite often, when transforming data, there are several stages of transformation, which when run one after another form what we call a \\"Data Pipeline\\" or a \\"Workflow\\". For example, a Workflow can start with an ingestion job, then feature several transformations that can fan-out and then fan-in. These Workflows can become rather complex in nature, and it leaves room for human error when individual steps are triggered manually or variability in a large team. In order to encourage reproducibility, it is encouraged to use a Workflow or Data Pipelining tool, in which individual jobs and its properties (like dependencies from which to trigger) are defined. These dependencies are often represented by a DAG (Directed acyclic graph). There are a variety of Workflow/Data Pipelining tools that exist such as Dagster, Airflow, and other Platform- or Cloud-native offerings.","sidebar":"tutorialSidebar"},"batch-processing/learn-more-about-the-spark-ui":{"id":"batch-processing/learn-more-about-the-spark-ui","title":"Want to learn more about the Spark UI? (Bonus)","description":"Data Engineering isn\'t just about transforming data, it\'s also about optimising for speed. The first step to optimising is to understand the Spark UI, which gives us an idea of how our data is transformed under the hood and where we can make improvements.","sidebar":"tutorialSidebar"},"batch-processing/multi-hop-medallion-domain-questions":{"id":"batch-processing/multi-hop-medallion-domain-questions","title":"The Multi-Hop/Medallion Architecture, Applied","description":"Recall that the Multi-Hop/Medallion Architecture comprises 3 stages: Bronze, Silver, and Gold.","sidebar":"tutorialSidebar"},"batch-processing/production-code-example":{"id":"batch-processing/production-code-example","title":"Production Code (Bonus)","description":"We\'ve spent a lot of time in notebooks and using notebooks as our main source of logic. In Production code, it\'s more encouraged to use code that is testable. If you\'re curious to know what the logic for the Bronze, Silver, Gold logic would look like in Production Code, check out this repo!","sidebar":"tutorialSidebar"},"batch-processing/quiz":{"id":"batch-processing/quiz","title":"Quiz","description":"What is one challenge of working with parquet as compared to csv with respect to debugging?","sidebar":"tutorialSidebar"},"batch-processing/spark-advanced-topics":{"id":"batch-processing/spark-advanced-topics","title":"Spark Advanced Topics (Bonus)","description":"Broadcasting","sidebar":"tutorialSidebar"},"batch-processing/spark-workflow-and-partitioning":{"id":"batch-processing/spark-workflow-and-partitioning","title":"Spark Workflow and Partitioning","description":"Optimisation","sidebar":"tutorialSidebar"},"batch-processing/udfs":{"id":"batch-processing/udfs","title":"UDFs (Bonus)","description":"What\'s missing?","sidebar":"tutorialSidebar"},"beyond-the-batch/checkpointing":{"id":"beyond-the-batch/checkpointing","title":"Checkpointing","description":"We now understand that stream processing applications in Spark run in an infinite loop of micro-batches. These applications run on a continuous basis on an unbounded set of real time data. We also know that any application in general cannot run for an infinite amount of time even if we want them to do that, for two practical reasons:","sidebar":"tutorialSidebar"},"beyond-the-batch/exercise-streaming":{"id":"beyond-the-batch/exercise-streaming","title":"Exercise: Streaming","description":"In this exercise, we\'ll put some of our Streaming concepts to practice! Follow the instructions here to get started!","sidebar":"tutorialSidebar"},"beyond-the-batch/handling-late-data":{"id":"beyond-the-batch/handling-late-data","title":"Handling Late Data","description":"We have seen that Spark Structured applications can store and aggregate data across micro-batches. Spark runtime stores this data in what Spark calls a statestore. This statestore uses the executor memory of the worker machines. Now a couple of questions have probably crossed your mind:","sidebar":"tutorialSidebar"},"beyond-the-batch/intro-to-streaming":{"id":"beyond-the-batch/intro-to-streaming","title":"Intro to Streaming","description":"What we have seen until now was only Spark SQL API which deals with batch use cases (scenarios where we deal with high volume, scheduled, high volume and repetitive jobs). Spark also has an API for streaming (i.e data in motion) use cases, called the Spark Structured Streaming API. Before we jump into the specifics of this API and see what it looks like, let us first understand why we do streaming, as in, what are the use cases which tells us that we have a streaming problem on hand.","sidebar":"tutorialSidebar"},"beyond-the-batch/sharing-streaming-data":{"id":"beyond-the-batch/sharing-streaming-data","title":"Sharing Streaming Data","description":"In the prior examples, we\'ve displayed our resulting aggregated stream data in the console. In the scenario where we would like to share the results of our aggregated streaming data with others, we can write that to a remote data store in the form of csv, json, parquet, and other formats similar to writing Batch data to remote data stores (Sink). It is also possible to write directly to Transactional databases, such as Cassandra or DynamoDB or even back to a Kafka topic. Spark supports writes to certain remote data stores out of the box (e.g. Cassandra) and some require leveraging the foreach operator plus some custom code to interact directly with the remote data store.","sidebar":"tutorialSidebar"},"beyond-the-batch/spark-structured-streaming":{"id":"beyond-the-batch/spark-structured-streaming","title":"Spark Structured Streaming","description":"Spark Structured Streaming is the streaming module of the Apache Spark framework. It provides a fast, scalable, fault-tolerant stream processing engine built on top of Spark SQL module with multi-delivery semantics. In fact, Spark Streaming APIs are an extension or superset of Spark SQL API. The module, by default, provides a low latency (with end-to-end latencies as low as 100 milliseconds) micro-batching features. Additionally, from Spark 2.3 onwards, a new low-latency processing mode called Continuous Processing was introduced, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees. Please note that Continuous mode is still experimental as of Spark 3.x.","sidebar":"tutorialSidebar"},"beyond-the-batch/stateful-streaming-nutshell":{"id":"beyond-the-batch/stateful-streaming-nutshell","title":"Stateful Streaming in a Nutshell","description":"stateful-streaming-in-a-nutshell.png","sidebar":"tutorialSidebar"},"beyond-the-batch/stateful-vs-stateless-streaming":{"id":"beyond-the-batch/stateful-vs-stateless-streaming","title":"Stateful vs. Stateless Streaming","description":"There are two types of transformation that can be done in Spark Structured Streaming known as Stateless and Stateful transformation. Our code can have both and in fact both types of transformations can work in tandem with each other. Let us take a look at an example below :","sidebar":"tutorialSidebar"},"beyond-the-batch/streaming-technologies":{"id":"beyond-the-batch/streaming-technologies","title":"Driving Streaming in Production","description":"We\'ve talked a lot about the theoretics (and even walked through some exercises) of Spark Streaming in this section. You might recall that many of our examples involved writing input to netcat, from which our Streaming Application picked up data. In production settings, we\'ll need to leverage tools that offer Pub/Sub architectures that are both addressable and can scale with lead. One popular tool is Kafka.","sidebar":"tutorialSidebar"},"data-mesh/intro":{"id":"data-mesh/intro","title":"Introduction","description":"","sidebar":"tutorialSidebar"},"data-mesh/journey-to-data-mesh":{"id":"data-mesh/journey-to-data-mesh","title":"Journey to Data Mesh (Optional)","description":"How we got to Data Mesh","sidebar":"tutorialSidebar"},"data-mesh/overview":{"id":"data-mesh/overview","title":"What is Data Mesh? (Optional)","description":"\u201cData mesh is a decentralized sociotechnical approach to share, access, and manage analytical data in complex and large-scale","sidebar":"tutorialSidebar"},"data-mesh/resources":{"id":"data-mesh/resources","title":"Want to learn more? (Optional)","description":"Foundational articles","sidebar":"tutorialSidebar"},"data-mesh/types-of-data":{"id":"data-mesh/types-of-data","title":"The analytical and operational planes (Optional)","description":"Throughout Data Mesh you\'ll hear a lot of references to analytical data and operational data. Looking back to the","sidebar":"tutorialSidebar"},"data-platforms/basics-of-a-data-platform":{"id":"data-platforms/basics-of-a-data-platform","title":"Basics of a Data Platform","description":"As demonstrated in the \\"My First Dataset\\" section, when answering multiple questions in the domain, it might be helpful to consolidate and share efforts so that other people who have a similar goal can benefit from your work. That can be at the infrastructure level, where a lightweight team might be focused on providing tooling to handle event streams, data storage, data catalogues (find your data), and additional common developer tooling (like libraries, compute power like Spark) that are relevant to consumers of the platform. At the level of data processing, teams working within the Platform might develop products that transform the data that can (a) answer a domain question (using varying degrees of Intelligence) to make a business decision or (b) eliminate the complexity of a certain data transformation that requires deep domain knowledge and serves that transformed data to another team for further processing.","sidebar":"tutorialSidebar"},"data-platforms/batch-vs-streaming":{"id":"data-platforms/batch-vs-streaming","title":"Batch vs. Streaming","description":"Because everyone asks, in the battle between batch vs. streaming, there is no clear winner, but rather we should choose to do batch or streaming based on what makes sense in our use case.","sidebar":"tutorialSidebar"},"data-platforms/landmarks-of-data-processing":{"id":"data-platforms/landmarks-of-data-processing","title":"Common Landmarks of Data Processing","description":"Once the scope of a Data Product has been defined and it is clear what question is to be answered and the data requirements (accuracy, freshness, etc) have been defined, we can start to think about the steps in Data Processing. It is necessary to understand the scope and requirements because it will help us understand what types of tech and processing that is required. For example, if the data is required to be near-real time, we might choose to transform less (saves time) and use streaming technologies. If the data is required to be processed once per month, we might choose to batch process our data (saves money).","sidebar":"tutorialSidebar"},"data-platforms/the-elements-of-a-data-product":{"id":"data-platforms/the-elements-of-a-data-product","title":"The Elements of a Data Product","description":"Data Engineers are often part of a team that is building a Data Product. If we recall, a Data Product aims to answer a specific domain question to help a business make a decision or transform data according to a complex domain construct so that others can reuse the output shape of data. Data Products can be in the form of a simple data transformation through to a more complex ML model. They are cross-functional teams, very closely involve the Consumer or Decision-maker, and employ many Software Development sensible defaults like iterative development, deploying small changes, testing, and continuous deployment.","sidebar":"tutorialSidebar"},"data-quality/data_reliability":{"id":"data-quality/data_reliability","title":"Data Reliability","description":"Data reliability is a critical aspect that should be intentionally built into every level of the organization, encompassing processes, technologies, communication, and issue triage. It plays a pivotal role in ensuring that the data used for decision-making is trustworthy and accurate.","sidebar":"tutorialSidebar"},"data-quality/dq_overwiew":{"id":"data-quality/dq_overwiew","title":"Data Quality","description":"Data quality rules are guidelines or criteria that define the standards and requirements for ensuring the accuracy, completeness, consistency, and integrity of data. Here is a list of commonly used data quality rules:","sidebar":"tutorialSidebar"},"data-science-and-interpretability/data-science-data-requirements":{"id":"data-science-and-interpretability/data-science-data-requirements","title":"Data Science Data Requirements","description":"Where an Analyst might ask for very specific or aggregated data (many cases, not all), a ML Engineer will more than likely ask for a subset of data but \\"as raw as possible\\". What they mean by this is that they want non-aggregated data as close to the source as possible. There is an extent to which some standardisation and harmonisation is ok (but it is important to document the transformations that have taken place as to not obscure any significant patterns that would have appeared in ML work).","sidebar":"tutorialSidebar"},"data-science-and-interpretability/overview":{"id":"data-science-and-interpretability/overview","title":"Jump into Data Science (Bonus)","description":"AI vs Machine Learning","sidebar":"tutorialSidebar"},"data-science-and-interpretability/want-to-learn-more":{"id":"data-science-and-interpretability/want-to-learn-more","title":"Want to learn more? (Bonus)","description":"andrew-ng.png","sidebar":"tutorialSidebar"},"data-visualisation/exercise":{"id":"data-visualisation/exercise","title":"Exercise","description":"Instructions","sidebar":"tutorialSidebar"},"data-visualisation/understanding-your-data":{"id":"data-visualisation/understanding-your-data","title":"Understanding Your Data","description":"Using a Critical Eye","sidebar":"tutorialSidebar"},"data-visualisation/visualization-tools":{"id":"data-visualisation/visualization-tools","title":"Visualization Tools","description":"Plotting and Visualization Libraries","sidebar":"tutorialSidebar"},"Intro-to-Data-Engineering/data-democratisation":{"id":"Intro-to-Data-Engineering/data-democratisation","title":"The Importance of Data Democratisation","description":"","sidebar":"tutorialSidebar"},"Intro-to-Data-Engineering/data-scientist-engineer-analyst":{"id":"Intro-to-Data-Engineering/data-scientist-engineer-analyst","title":"Data Science vs. Data Engineer vs. Data Analyst","description":"Read: Data Engineer, Data Analyst, Data Scientist \u2014 What\u2019s the Difference?","sidebar":"tutorialSidebar"},"Intro-to-Data-Engineering/what-is-data-engineering":{"id":"Intro-to-Data-Engineering/what-is-data-engineering","title":"What is Data Engineering?","description":"","sidebar":"tutorialSidebar"},"Real-time-Problem-Statement/exercise-final-charge-time-dispense":{"id":"Real-time-Problem-Statement/exercise-final-charge-time-dispense","title":"Exercise: Final Charge Time and Charge Dispense for Completed Charges","description":"| Question | Context |","sidebar":"tutorialSidebar"},"Real-time-Problem-Statement/exercise-last-connection-charge-points":{"id":"Real-time-Problem-Statement/exercise-last-connection-charge-points","title":"Exercise: Last Connection Time of Charge Points","description":"| Question                                                 | Context                                                                                                                                                                                                                                                                                         |","sidebar":"tutorialSidebar"},"Real-time-Problem-Statement/Problem-statement":{"id":"Real-time-Problem-Statement/Problem-statement","title":"Problem Statement \ud83d\udca1","description":"Now that we\'ve covered the theory behind Data Engineering, we\'ll spend the next few sections putting that to practice in the context of a domain. We\'ll do that by asking some relevant domain questions, walking through the data processing steps and architectures required to answer those questions, and then implementing them in exercises for the rest of the training.","sidebar":"tutorialSidebar"},"Real-time-Problem-Statement/reflect":{"id":"Real-time-Problem-Statement/reflect","title":"Reflect","description":"Congratulations on finishing the exercises! You\'ve learned how to take some data, a question, and transform that using the Spark API into something that represents an answer to that question.","sidebar":"tutorialSidebar"},"SQL/advance-sql":{"id":"SQL/advance-sql","title":"Advanced SQL: Window and Aggregate Functions","description":"There are a lot of interesting functions to transform and add properties to your dataset","sidebar":"tutorialSidebar"},"SQL/sql-quick-review":{"id":"SQL/sql-quick-review","title":"SQL: A Quick Review","description":"SQL is a programming language used to manage and manipulate relational databases. It is used to create, read, update, and delete data in a database. SQL is widely used in data analysis, business intelligence, and web development. Its syntax is simple and easy to understand, making it a popular choice for data professionals.","sidebar":"tutorialSidebar"},"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-formats":{"id":"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-formats","title":"Data Formats","description":"Wait...what\u2019s data serialization again?","sidebar":"tutorialSidebar"},"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-1":{"id":"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-1","title":"Data Milky Way: A Brief History (Part 1) - OLTP vs. OLAP","description":"History belongs in the past; but understanding it is the duty of the present","sidebar":"tutorialSidebar"},"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-2-evolution":{"id":"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-2-evolution","title":"Data Milky Way: A Brief History (Part 2) - Evolution","description":"1980s: Databases & Data Warehouses","sidebar":"tutorialSidebar"},"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-3-data-processing":{"id":"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-3-data-processing","title":"Data Milky Way: Brief History (Part 3) - Data Processing","description":"Evolution of Data Processing","sidebar":"tutorialSidebar"},"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-4-arch-ref":{"id":"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-brief-history-part-4-arch-ref","title":"Data Milky Way: A Brief History (Part 4) - Architecture Reference","description":"Typical Data Pipeline","sidebar":"tutorialSidebar"},"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-nosql-cap-theorem":{"id":"The-Art-of-Data-Engineering:Crafting-Robust-and-Scalable-Solutions/data-milky-way-nosql-cap-theorem","title":"Data Milky Way: Distributed Data Systems (NoSQL & CAP)","description":"We talked a little about NoSQL earlier. Let\'s just skim the surface and talk about some important concepts driving it.","sidebar":"tutorialSidebar"},"tour_agenda/tour_agenda":{"id":"tour_agenda/tour_agenda","title":"Tour Agenda","description":"dataawareagenda.png","sidebar":"tutorialSidebar"}}}')}}]);